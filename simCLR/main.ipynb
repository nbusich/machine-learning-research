{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.510144Z",
     "start_time": "2025-04-09T00:03:15.479897Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from thop import profile, clever_format\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models.resnet import resnet50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.531094Z",
     "start_time": "2025-04-09T00:03:15.523508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sim(z_i, z_j):\n",
    "    \n",
    "    \"\"\"\n",
    "    Normalized dot product between two vectors.\n",
    "\n",
    "    Inputs:\n",
    "    - z_i: 1xD tensor.\n",
    "    - z_j: 1xD tensor.\n",
    "    \n",
    "    Returns:\n",
    "    - A scalar value that is the normalized dot product between z_i and z_j.\n",
    "    \"\"\"\n",
    "    \n",
    "    norm_dot_product = None\n",
    "    num = z_i @ z_j\n",
    "    denom = torch.linalg.norm(z_i) * torch.linalg.norm(z_j)\n",
    "    norm_dot_product = num/denom\n",
    "    \n",
    "    return norm_dot_product"
   ],
   "id": "ff35ce2489fa76cb",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.551459Z",
     "start_time": "2025-04-09T00:03:15.546963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sim_positive_pairs(out_left, out_right):\n",
    "    \"\"\"Normalized dot product between positive pairs.\n",
    "\n",
    "    Inputs:\n",
    "    - out_left: NxD tensor; output of the projection head g(), left branch in SimCLR model.\n",
    "    - out_right: NxD tensor; output of the projection head g(), right branch in SimCLR model.\n",
    "    Each row is a z-vector for an augmented sample in the batch.\n",
    "    The same row in out_left and out_right form a positive pair.\n",
    "    \n",
    "    Returns:\n",
    "    - A Nx1 tensor; each row k is the normalized dot product between out_left[k] and out_right[k].\n",
    "    \"\"\"\n",
    "    pos_pairs = None\n",
    "    \n",
    "    # Compute the normalized head projection outputs over each row of features\n",
    "    norm_left = out_left / torch.linalg.norm(out_left, dim=1, keepdim=True)\n",
    "    norm_right = out_right / torch.linalg.norm(out_right, dim=1, keepdim=True)\n",
    "\n",
    "    # Compute the diagonal dot product directly by multiplying and summing\n",
    "    pos_pairs = (norm_left * norm_right).sum(dim=1, keepdim=True)\n",
    "    \n",
    "    return pos_pairs"
   ],
   "id": "6b221f3263c3b7cc",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.562212Z",
     "start_time": "2025-04-09T00:03:15.559471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_sim_matrix(out):\n",
    "    \"\"\"Compute a 2N x 2N matrix of normalized dot products between all pairs of augmented examples in a batch.\n",
    "\n",
    "    Inputs:\n",
    "    - out: 2N x D tensor; each row is the z-vector (output of projection head) of a single augmented example.\n",
    "    There are a total of 2N augmented examples in the batch.\n",
    "    \n",
    "    Returns:\n",
    "    - sim_matrix: 2N x 2N tensor; each element i, j in the matrix is the normalized dot product between out[i] and out[j].\n",
    "    \"\"\"\n",
    "    sim_matrix = None\n",
    "    \n",
    "    norm_out = out / torch.linalg.norm(out, dim=1, keepdim=True)\n",
    "    sim_matrix = norm_out @ norm_out.T\n",
    "    print(sim_matrix.shape)\n",
    "\n",
    "    return sim_matrix"
   ],
   "id": "6416da8b3c031e66",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.577049Z",
     "start_time": "2025-04-09T00:03:15.570601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def simclr_loss_vectorized(out_left, out_right, tau, device=device):\n",
    "    \"\"\"Compute the contrastive loss L over a batch (vectorized version). No loops are allowed.\n",
    "    \n",
    "    Inputs and output are the same as in simclr_loss_naive.\n",
    "    \"\"\"\n",
    "    N = out_left.shape[0]\n",
    "    \n",
    "    # Concatenate out_left and out_right into a 2*N x D tensor.\n",
    "    out = torch.cat([out_left, out_right], dim=0)  # [2*N, D]\n",
    "    \n",
    "    # Compute similarity matrix between all pairs of augmented examples in the batch.\n",
    "    sim_matrix = compute_sim_matrix(out)  # [2*N, 2*N]\n",
    "    \n",
    "    # Step 1: Use sim_matrix to compute the denominator value for all augmented samples.\n",
    "    exponential = torch.exp(sim_matrix/tau)\n",
    "    \n",
    "    # This binary mask zeros out terms where k=i.\n",
    "    mask = (torch.ones_like(exponential, device=device) - torch.eye(2 * N, device=device)).to(device).bool()\n",
    "    \n",
    "    # We apply the binary mask.\n",
    "    exponential = exponential.masked_select(mask).view(2 * N, -1)  # [2*N, 2*N-1]\n",
    "    \n",
    "    # Hint: Compute the denominator values for all augmented samples. This should be a 2N x 1 vector.\n",
    "    denom = torch.sum(exponential, axis = 1)\n",
    "    \n",
    "    # Step 2: Compute similarity between positive pairs.\n",
    "    # You can do this in two ways: \n",
    "    # Option 1: Extract the corresponding indices from sim_matrix. \n",
    "    # Option 2: Use sim_positive_pairs().\n",
    "    similarity_vector = sim_positive_pairs(out_left, out_right) # shape (2,1)\n",
    "    \n",
    "    # Step 3: Compute the numerator value for all augmented samples.\n",
    "    numerator = None\n",
    "    numerator = torch.exp(similarity_vector/tau)\n",
    "    \n",
    "    # Step 4: Now that you have the numerator and denominator for all augmented samples, compute the total loss.\n",
    "    loss = -torch.log(numerator/denom).mean()\n",
    "    return loss"
   ],
   "id": "2408b31aa743fead",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.586339Z",
     "start_time": "2025-04-09T00:03:15.583272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_train_transform(seed=123456):\n",
    "    \"\"\"\n",
    "    This function returns a composition of train augmentations to a single training image.\n",
    "    Complete the following lines. Hint: look at available functions in torchvision.transforms\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    \n",
    "    # Transformation that applies color jitter with brightness=0.4, contrast=0.4, saturation=0.4, and hue=0.1\n",
    "    color_jitter = transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  \n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        \n",
    "        # Step 1: Randomly resize and crop to 32x32.\n",
    "        transforms.RandomResizedCrop(size = (32,32)),\n",
    "\n",
    "        # Step 2: Horizontally flip the image with probability 0.5\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "        # Step 3: With a probability of 0.8, apply color jitter (you can use \"color_jitter\" defined above.\n",
    "        transforms.RandomApply([color_jitter],p=0.8),\n",
    "\n",
    "        # Step 4: With a probability of 0.2, convert the image to grayscale\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "    return train_transform"
   ],
   "id": "b4c9dfeeabdd40ac",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.597951Z",
     "start_time": "2025-04-09T00:03:15.592629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Lyme40(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: path to the root folder of your data.\n",
    "                  The folder structure should be:\n",
    "                  root_dir/\n",
    "                    train/\n",
    "                      good/\n",
    "                      bad/\n",
    "                    test/\n",
    "                      good/\n",
    "                      bad/\n",
    "        split: 'train' or 'test' to select the proper folder.\n",
    "        transform: transforms to apply to the images (for x_i, x_j)\n",
    "        target_transform: transforms to apply to the label (optional)\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # Build folder paths based on split\n",
    "        good_dir = os.path.join(root_dir, split, \"good\")\n",
    "        bad_dir = os.path.join(root_dir, split, \"bad\")\n",
    "\n",
    "        # Initialize lists to hold file paths and labels\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Gather images labeled as \"good\" = 0\n",
    "        for img_path in glob.glob(os.path.join(good_dir, \"*\")):\n",
    "            self.data.append(img_path)\n",
    "            self.targets.append(0)\n",
    "\n",
    "        # Gather images labeled as \"bad\" = 1\n",
    "        for img_path in glob.glob(os.path.join(bad_dir, \"*\")):\n",
    "            self.data.append(img_path)\n",
    "            self.targets.append(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        # Read the image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Generate two augmented views (x_i, x_j) if transform is provided\n",
    "        if self.transform is not None:\n",
    "            x_i = self.transform(image)\n",
    "            x_j = self.transform(image)\n",
    "        else:\n",
    "            x_i, x_j = image, image\n",
    "\n",
    "        # Optionally transform the target label\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return x_i, x_j, target"
   ],
   "id": "59ea44d43ab50c6c",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.604729Z",
     "start_time": "2025-04-09T00:03:15.603048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class CIFAR10Pair(CIFAR10):\n",
    "#     \"\"\"\n",
    "#     CIFAR10 Dataset.\n",
    "#     \"\"\"\n",
    "#     def __getitem__(self, index):\n",
    "#         img, target = self.data[index], self.targets[index]\n",
    "#         img = Image.fromarray(img)\n",
    "# \n",
    "#         x_i = None\n",
    "#         x_j = None\n",
    "# \n",
    "#         if self.transform is not None:\n",
    "#             \n",
    "#             x_i = self.transform(img)\n",
    "#             x_j = self.transform(img)\n",
    "# \n",
    "#         if self.target_transform is not None:\n",
    "#             target = self.target_transform(target)\n",
    "# \n",
    "#         return x_i, x_j, target"
   ],
   "id": "845dd943fd60e29f",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.611699Z",
     "start_time": "2025-04-09T00:03:15.609478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_test_transform():\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "    return test_transform"
   ],
   "id": "286a3802d419f4d5",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.619533Z",
     "start_time": "2025-04-09T00:03:15.616576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.f = []\n",
    "        for name, module in resnet50().named_children():\n",
    "            if name == 'conv1':\n",
    "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n",
    "                self.f.append(module)\n",
    "        # encoder\n",
    "        self.f = nn.Sequential(*self.f)\n",
    "        # projection head\n",
    "        self.g = nn.Sequential(nn.Linear(2048, 512, bias=False), nn.BatchNorm1d(512),\n",
    "                               nn.ReLU(inplace=True), nn.Linear(512, feature_dim, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        out = self.g(feature)\n",
    "        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)"
   ],
   "id": "5b37e943876dff52",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.626111Z",
     "start_time": "2025-04-09T00:03:15.623979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # Encoder.\n",
    "        self.f = Model().f\n",
    "\n",
    "        # Classifier.\n",
    "        self.fc = nn.Linear(2048, num_class, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        out = self.fc(feature)\n",
    "        return out"
   ],
   "id": "78dbb11fcfe497d5",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.641388Z",
     "start_time": "2025-04-09T00:03:15.635616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, data_loader, train_optimizer, epoch, epochs, batch_size=32, temperature=0.5, device=device):\n",
    "    \"\"\"Trains the model defined in ./model.py with one epoch.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: Model class object as defined in ./model.py.\n",
    "    - data_loader: torch.utils.train.DataLoader object; loads in training train. You can assume the loaded train has been augmented.\n",
    "    - train_optimizer: torch.optim.Optimizer object; applies an optimizer to training.\n",
    "    - epoch: integer; current epoch number.\n",
    "    - epochs: integer; total number of epochs.\n",
    "    - batch_size: Number of training samples per batch.\n",
    "    - temperature: float; temperature (tau) parameter used in simclr_loss_vectorized.\n",
    "    - device: the device name to define torch tensors.\n",
    "\n",
    "    Returns:\n",
    "    - The average loss.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
    "    for data_pair in train_bar:\n",
    "        x_i, x_j, target = data_pair\n",
    "        x_i, x_j = x_i.to(device), x_j.to(device)\n",
    "        \n",
    "        out_left, out_right, loss = None, None, None\n",
    "        \n",
    "        _,out_left = model(x_i)\n",
    "        _,out_right = model(x_j)\n",
    "        \n",
    "        loss = simclr_loss_vectorized(out_left, out_right, temperature, device=device)\n",
    "        \n",
    "        train_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_optimizer.step()\n",
    "\n",
    "        total_num += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f}'.format(epoch, epochs, total_loss / total_num))\n",
    "\n",
    "    return total_loss / total_num"
   ],
   "id": "10796da42539cec6",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.656531Z",
     "start_time": "2025-04-09T00:03:15.650465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def train_val(model, data_loader, train_optimizer, epoch, epochs, device=device):\n",
    "#     is_train = train_optimizer is not None\n",
    "#     model.train() if is_train else model.eval()\n",
    "#     loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "# \n",
    "#     total_loss, total_correct_1, total_correct_5, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader)\n",
    "#     with (torch.enable_grad() if is_train else torch.no_grad()):\n",
    "#         for data, target in data_bar:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             out = model(data)\n",
    "#             loss = loss_criterion(out, target)\n",
    "# \n",
    "#             if is_train:\n",
    "#                 train_optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 train_optimizer.step()\n",
    "# \n",
    "#             total_num += data.size(0)\n",
    "#             total_loss += loss.item() * data.size(0)\n",
    "#             prediction = torch.argsort(out, dim=-1, descending=True)\n",
    "#             total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "#             total_correct_5 += torch.sum((prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "# \n",
    "#             data_bar.set_description('{} Epoch: [{}/{}] Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n",
    "#                                      .format('Train' if is_train else 'Test', epoch, epochs, total_loss / total_num,\n",
    "#                                              total_correct_1 / total_num * 100, total_correct_5 / total_num * 100))\n",
    "# \n",
    "#     return total_loss / total_num, total_correct_1 / total_num * 100, total_correct_5 / total_num * 100\n"
   ],
   "id": "5a7462c1ca2e6c6d",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:25:49.390305Z",
     "start_time": "2025-04-09T00:25:49.373325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_val(model, data_loader, train_optimizer, epoch, epochs, device=device):\n",
    "    is_train = train_optimizer is not None\n",
    "    # Set the mode of the model: train for training, eval otherwise.\n",
    "    model.train() if is_train else model.eval()\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss, total_correct_1, total_correct_5, total_num = 0.0, 0.0, 0.0, 0\n",
    "    data_bar = tqdm(data_loader)\n",
    "    # Use grad if training; disable during evaluation.\n",
    "    with (torch.enable_grad() if is_train else torch.no_grad()):\n",
    "        # Unpack three values per batch: two augmented views and the target.\n",
    "        for x_i, x_j, target in data_bar:\n",
    "            # For a linear evaluation, we can use just one view (x_i).\n",
    "            x_i, target = x_i.to(device), target.to(device)\n",
    "            # Forward pass of the model.\n",
    "            out = model(x_i)\n",
    "            loss = loss_criterion(out, target)\n",
    "\n",
    "            if is_train:\n",
    "                train_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                train_optimizer.step()\n",
    "\n",
    "            total_num += x_i.size(0)\n",
    "            total_loss += loss.item() * x_i.size(0)\n",
    "            \n",
    "            # Compute predictions: descending sort gives us the indices of classes.\n",
    "            prediction = torch.argsort(out, dim=-1, descending=True)\n",
    "            total_correct_1 += torch.sum(\n",
    "                (prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()\n",
    "            ).item()\n",
    "            total_correct_5 += torch.sum(\n",
    "                (prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()\n",
    "            ).item()\n",
    "\n",
    "            data_bar.set_description('{} Epoch: [{}/{}] Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n",
    "                .format('Train' if is_train else 'Test', epoch, epochs,\n",
    "                        total_loss / total_num,\n",
    "                        total_correct_1 / total_num * 100,\n",
    "                        total_correct_5 / total_num * 100))\n",
    "\n",
    "    return total_loss / total_num, total_correct_1 / total_num * 100, total_correct_5 / total_num * 100"
   ],
   "id": "b7a55c960c9d5af5",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:03:15.667385Z",
     "start_time": "2025-04-09T00:03:15.661724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model, memory_data_loader, test_data_loader, epoch, epochs, c, temperature=0.5, k=200, device=device):\n",
    "    model.eval()\n",
    "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
    "    with torch.no_grad():\n",
    "        # generate feature bank\n",
    "        for data, _, target in tqdm(memory_data_loader, desc='Feature extracting'):\n",
    "            feature, out = model(data.to(device))\n",
    "            feature_bank.append(feature)\n",
    "        # [D, N]\n",
    "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
    "        # [N]\n",
    "        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n",
    "        # loop test train to predict the label by weighted knn search\n",
    "        test_bar = tqdm(test_data_loader)\n",
    "        for data, _, target in test_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            feature, out = model(data)\n",
    "\n",
    "            total_num += data.size(0)\n",
    "            # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
    "            sim_matrix = torch.mm(feature, feature_bank)\n",
    "            \n",
    "            # [B, K]\n",
    "            sim_weight, sim_indices = sim_matrix.topk(k=k, dim=-1)\n",
    "            # [B, K]\n",
    "            sim_labels = torch.gather(feature_labels.expand(data.size(0), -1), dim=-1, index=sim_indices)\n",
    "            sim_weight = (sim_weight / temperature).exp()\n",
    "\n",
    "            # counts for each class\n",
    "            one_hot_label = torch.zeros(data.size(0) * k, c, device=device)\n",
    "            # [B*K, C]\n",
    "            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
    "            # weighted score ---> [B, C]\n",
    "            pred_scores = torch.sum(one_hot_label.view(data.size(0), -1, c) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
    "\n",
    "            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
    "            total_top1 += torch.sum((pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "            total_top5 += torch.sum((pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}% Acc@5:{:.2f}%'\n",
    "                                     .format(epoch, epochs, total_top1 / total_num * 100, total_top5 / total_num * 100))\n",
    "\n",
    "    return total_top1 / total_num * 100, total_top5 / total_num * 100"
   ],
   "id": "19ab5e37c0d4a938",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:04:58.806030Z",
     "start_time": "2025-04-09T00:03:15.684391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_dim = 128\n",
    "temperature = 0.5\n",
    "k = 5\n",
    "batch_size = 5\n",
    "epochs = 1\n",
    "temperature = 0.5\n",
    "percentage = 0.5\n",
    "pretrained_path = 'pretrained_model/pretrained_simclr_model.pth'\n",
    "root_data_dir = 'Lyme40'\n",
    "num_workers = 0\n",
    "\n",
    "# Prepare the data.\n",
    "train_transform = compute_train_transform()\n",
    "test_transform = compute_test_transform()\n",
    "\n",
    "train_data = Lyme40(root_data_dir, split='train', transform=train_transform, target_transform=None)\n",
    "train_data = torch.utils.data.Subset(train_data, list(np.arange(int(len(train_data)*percentage))))\n",
    "memory_data = Lyme40(root_data_dir, split='train', transform=train_transform, target_transform=None)\n",
    "test_data = Lyme40(root_data_dir, split='test', transform=test_transform, target_transform=None)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# Set up the model and optimizer config.\n",
    "model = Model(feature_dim)\n",
    "model.load_state_dict(torch.load(pretrained_path, map_location='cpu'), strict=False)\n",
    "model = model.to(device)\n",
    "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).to(device),))\n",
    "flops, params = clever_format([flops, params])\n",
    "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "c = 2\n",
    "\n",
    "# Training loop.\n",
    "results = {'train_loss': [], 'test_acc@1': [], 'test_acc@5': []} #<< -- output\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, epoch, epochs, batch_size=batch_size, temperature=temperature, device=device)\n",
    "    results['train_loss'].append(train_loss)\n",
    "    test_acc_1, test_acc_5 = test(model, memory_loader, test_loader, epoch, epochs, c, k=k, temperature=temperature, device=device)\n",
    "    results['test_acc@1'].append(test_acc_1)\n",
    "    results['test_acc@5'].append(test_acc_5)\n",
    "\n",
    "    # Save statistics.\n",
    "    if test_acc_1 > best_acc:\n",
    "        best_acc = test_acc_1\n",
    "        torch.save(model.state_dict(), './pretrained_model/trained_simclr_model.pth')"
   ],
   "id": "3ae26130ad83bbb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "# Model Params: 24.62M FLOPs: 1.31G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [1/1] Loss: 1.3484:  17%|█▋        | 1/6 [00:00<00:03,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [1/1] Loss: 1.4476:  33%|███▎      | 2/6 [00:01<00:02,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [1/1] Loss: 1.5002:  50%|█████     | 3/6 [00:01<00:01,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [1/1] Loss: 1.6228:  67%|██████▋   | 4/6 [00:02<00:01,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [1/1] Loss: 1.7893:  83%|████████▎ | 5/6 [00:02<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [1/1] Loss: 1.8216: 100%|██████████| 6/6 [00:03<00:00,  1.74it/s]\n",
      "Feature extracting: 100%|██████████| 13/13 [00:01<00:00,  8.26it/s]\n",
      "Test Epoch: [1/1] Acc@1:50.00% Acc@5:100.00%: 100%|██████████| 4/4 [01:37<00:00, 24.26s/it]\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:03:02.951216Z",
     "start_time": "2025-04-09T13:03:02.907024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Parameters:\n",
    "feature_dim = 64\n",
    "temperature = 0.5\n",
    "k = 118\n",
    "batch_size = 60\n",
    "epochs = 10\n",
    "percentage = 0.8\n",
    "pretrained_path = 'pretrained_model/pretrained_simclr_model.pth'\n",
    "num_workers = 0\n",
    "root_data_dir = 'Lyme40'\n",
    "map_location = 'cpu'\n",
    "lr = 1e-3\n",
    "weight_decay=1e-6\n",
    "number_of_classes = 2"
   ],
   "id": "fe8bb3b7425c0a19",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:03:05.593514Z",
     "start_time": "2025-04-09T13:03:05.535333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading custom dataset, Lyme40, using built in pytorch methods\n",
    "train_transform = compute_train_transform() # These define the transforms that are carried out on each image\n",
    "test_transform = compute_test_transform()\n",
    "\n",
    "# Splitting data into test and train according to folder structure\n",
    "train_data = Lyme40(root_data_dir, split='train', transform=train_transform, target_transform=None)\n",
    "test_data =  Lyme40(root_data_dir, split='test', transform=test_transform, target_transform=None)\n",
    "\n",
    "# Taking a subset of the training dataset\n",
    "trainset = torch.utils.data.Subset(train_data, list(np.arange(int(len(train_data) * percentage))))\n",
    "\n",
    "# Moving the train into a dataloader to make it easy to get images and labels\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "test_loader =  DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ],
   "id": "bb2bb932ef3ee0eb",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:04:58.861694Z",
     "start_time": "2025-04-09T00:04:58.859800Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "431acf715aca62ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T00:43:12.290187Z",
     "start_time": "2025-04-09T00:25:56.422716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Classifier(num_class=number_of_classes)\n",
    "model.load_state_dict(torch.load(pretrained_path, map_location= map_location), strict=False)\n",
    "model = model.to(device)\n",
    "for param in model.f.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).to(device),))\n",
    "flops, params = clever_format([flops, params])\n",
    "\n",
    "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "pretrain_results = {'train_loss': [], 'train_acc@1': [], 'train_acc@5': [],\n",
    "                    'test_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_acc_1, train_acc_5 = train_val(model, train_loader, optimizer, epoch, epochs)\n",
    "    pretrain_results['train_loss'].append(train_loss)\n",
    "    pretrain_results['train_acc@1'].append(train_acc_1)\n",
    "    pretrain_results['train_acc@5'].append(train_acc_5)\n",
    "    test_loss, test_acc_1, test_acc_5 = train_val(model, test_loader, None, epoch, epochs)\n",
    "    pretrain_results['test_loss'].append(test_loss)\n",
    "    pretrain_results['test_acc@1'].append(test_acc_1)\n",
    "    pretrain_results['test_acc@5'].append(test_acc_5)\n",
    "    if test_acc_1 > best_acc:\n",
    "        best_acc = test_acc_1\n",
    "\n",
    "# Print the best test accuracy. You should see a best top-1 accuracy of >=70%.\n",
    "print('Best top-1 accuracy with self-supervised learning: ', best_acc)"
   ],
   "id": "1e8bfab386f994f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "# Model Params: 23.50M FLOPs: 1.31G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [1/10] Loss: 0.6393 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [1/10] Loss: 0.6393 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  3.65it/s]\u001B[A\n",
      "Train Epoch: [1/10] Loss: 0.6106 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  3.65it/s]\u001B[A\n",
      "Train Epoch: [1/10] Loss: 0.6106 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.43it/s]\u001B[A\n",
      "Train Epoch: [1/10] Loss: 0.5788 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.43it/s]\u001B[A\n",
      "Train Epoch: [1/10] Loss: 0.5788 ACC@1: 100.00% ACC@5: 100.00%:  75%|███████▌  | 3/4 [00:00<00:00,  4.64it/s]\u001B[A\n",
      "Train Epoch: [1/10] Loss: 0.5718 ACC@1: 100.00% ACC@5: 100.00%: 100%|██████████| 4/4 [00:00<00:00,  5.43it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [1/10] Loss: 0.6146 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/2 [01:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [1/10] Loss: 0.6146 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:00<01:00, 60.05s/it]\u001B[A\n",
      "Test Epoch: [1/10] Loss: 0.6997 ACC@1: 50.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:51<01:00, 60.05s/it] \u001B[A\n",
      "Test Epoch: [1/10] Loss: 0.6997 ACC@1: 50.00% ACC@5: 100.00%: 100%|██████████| 2/2 [01:51<00:00, 55.57s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [2/10] Loss: 0.4191 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [2/10] Loss: 0.4191 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.35it/s]\u001B[A\n",
      "Train Epoch: [2/10] Loss: 0.3939 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.35it/s]\u001B[A\n",
      "Train Epoch: [2/10] Loss: 0.3939 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.99it/s]\u001B[A\n",
      "Train Epoch: [2/10] Loss: 0.3744 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.99it/s]\u001B[A\n",
      "Train Epoch: [2/10] Loss: 0.3744 ACC@1: 100.00% ACC@5: 100.00%:  75%|███████▌  | 3/4 [00:00<00:00,  5.14it/s]\u001B[A\n",
      "Train Epoch: [2/10] Loss: 0.3704 ACC@1: 100.00% ACC@5: 100.00%: 100%|██████████| 4/4 [00:00<00:00,  5.94it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [2/10] Loss: 0.5140 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/2 [00:50<?, ?it/s]\u001B[A\n",
      "Test Epoch: [2/10] Loss: 0.5140 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 1/2 [00:50<00:50, 51.00s/it]\u001B[A\n",
      "Test Epoch: [2/10] Loss: 0.7159 ACC@1: 50.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:43<00:50, 51.00s/it] \u001B[A\n",
      "Test Epoch: [2/10] Loss: 0.7159 ACC@1: 50.00% ACC@5: 100.00%: 100%|██████████| 2/2 [01:43<00:00, 51.76s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [3/10] Loss: 0.2653 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [3/10] Loss: 0.2653 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.10it/s]\u001B[A\n",
      "Train Epoch: [3/10] Loss: 0.2511 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.10it/s]\u001B[A\n",
      "Train Epoch: [3/10] Loss: 0.2511 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.62it/s]\u001B[A\n",
      "Train Epoch: [3/10] Loss: 0.2384 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.62it/s]\u001B[A\n",
      "Train Epoch: [3/10] Loss: 0.2384 ACC@1: 100.00% ACC@5: 100.00%:  75%|███████▌  | 3/4 [00:00<00:00,  4.65it/s]\u001B[A\n",
      "Train Epoch: [3/10] Loss: 0.2358 ACC@1: 100.00% ACC@5: 100.00%: 100%|██████████| 4/4 [00:00<00:00,  5.50it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [3/10] Loss: 0.4443 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/2 [00:55<?, ?it/s]\u001B[A\n",
      "Test Epoch: [3/10] Loss: 0.4443 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 1/2 [00:55<00:55, 55.75s/it]\u001B[A\n",
      "Test Epoch: [3/10] Loss: 0.7375 ACC@1: 50.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:47<00:55, 55.75s/it] \u001B[A\n",
      "Test Epoch: [3/10] Loss: 0.7375 ACC@1: 50.00% ACC@5: 100.00%: 100%|██████████| 2/2 [01:47<00:00, 53.63s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [4/10] Loss: 0.1688 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [4/10] Loss: 0.1688 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.30it/s]\u001B[A\n",
      "Train Epoch: [4/10] Loss: 0.1619 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.30it/s]\u001B[A\n",
      "Train Epoch: [4/10] Loss: 0.1619 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.78it/s]\u001B[A\n",
      "Train Epoch: [4/10] Loss: 0.1542 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.78it/s]\u001B[A\n",
      "Train Epoch: [4/10] Loss: 0.1542 ACC@1: 100.00% ACC@5: 100.00%:  75%|███████▌  | 3/4 [00:00<00:00,  4.93it/s]\u001B[A\n",
      "Train Epoch: [4/10] Loss: 0.1525 ACC@1: 100.00% ACC@5: 100.00%: 100%|██████████| 4/4 [00:00<00:00,  5.97it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [4/10] Loss: 0.3891 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/2 [00:50<?, ?it/s]\u001B[A\n",
      "Test Epoch: [4/10] Loss: 0.3891 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 1/2 [00:50<00:50, 50.24s/it]\u001B[A\n",
      "Test Epoch: [4/10] Loss: 0.7634 ACC@1: 50.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:40<00:50, 50.24s/it] \u001B[A\n",
      "Test Epoch: [4/10] Loss: 0.7634 ACC@1: 50.00% ACC@5: 100.00%: 100%|██████████| 2/2 [01:40<00:00, 50.31s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [5/10] Loss: 0.1145 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [5/10] Loss: 0.1145 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  5.11it/s]\u001B[A\n",
      "Train Epoch: [5/10] Loss: 0.1108 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  5.11it/s]\u001B[A\n",
      "Train Epoch: [5/10] Loss: 0.1108 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  5.24it/s]\u001B[A\n",
      "Train Epoch: [5/10] Loss: 0.1056 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  5.24it/s]\u001B[A\n",
      "Train Epoch: [5/10] Loss: 0.1056 ACC@1: 100.00% ACC@5: 100.00%:  75%|███████▌  | 3/4 [00:00<00:00,  5.39it/s]\u001B[A\n",
      "Train Epoch: [5/10] Loss: 0.1051 ACC@1: 100.00% ACC@5: 100.00%: 100%|██████████| 4/4 [00:00<00:00,  6.26it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [5/10] Loss: 0.3514 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/2 [00:49<?, ?it/s]\u001B[A\n",
      "Test Epoch: [5/10] Loss: 0.3514 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 1/2 [00:49<00:49, 49.92s/it]\u001B[A\n",
      "Test Epoch: [5/10] Loss: 0.7869 ACC@1: 50.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:40<00:49, 49.92s/it] \u001B[A\n",
      "Test Epoch: [5/10] Loss: 0.7869 ACC@1: 50.00% ACC@5: 100.00%: 100%|██████████| 2/2 [01:40<00:00, 50.08s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [6/10] Loss: 0.0777 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [6/10] Loss: 0.0777 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.84it/s]\u001B[A\n",
      "Train Epoch: [6/10] Loss: 0.0755 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.84it/s]\u001B[A\n",
      "Train Epoch: [6/10] Loss: 0.0755 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  5.11it/s]\u001B[A\n",
      "Train Epoch: [6/10] Loss: 0.0739 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  5.11it/s]\u001B[A\n",
      "Train Epoch: [6/10] Loss: 0.0739 ACC@1: 100.00% ACC@5: 100.00%:  75%|███████▌  | 3/4 [00:00<00:00,  5.02it/s]\u001B[A\n",
      "Train Epoch: [6/10] Loss: 0.0733 ACC@1: 100.00% ACC@5: 100.00%: 100%|██████████| 4/4 [00:00<00:00,  6.06it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [6/10] Loss: 0.3206 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/2 [00:50<?, ?it/s]\u001B[A\n",
      "Test Epoch: [6/10] Loss: 0.3206 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 1/2 [00:50<00:50, 50.17s/it]\u001B[A\n",
      "Test Epoch: [6/10] Loss: 0.8117 ACC@1: 50.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:39<00:50, 50.17s/it] \u001B[A\n",
      "Test Epoch: [6/10] Loss: 0.8117 ACC@1: 50.00% ACC@5: 100.00%: 100%|██████████| 2/2 [01:39<00:00, 49.91s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [7/10] Loss: 0.0590 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [7/10] Loss: 0.0590 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.50it/s]\u001B[A\n",
      "Train Epoch: [7/10] Loss: 0.0592 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.50it/s]\u001B[A\n",
      "Train Epoch: [7/10] Loss: 0.0592 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.59it/s]\u001B[A\n",
      "Train Epoch: [7/10] Loss: 0.0576 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.59it/s]\u001B[A\n",
      "Train Epoch: [7/10] Loss: 0.0576 ACC@1: 100.00% ACC@5: 100.00%:  75%|███████▌  | 3/4 [00:00<00:00,  4.74it/s]\u001B[A\n",
      "Train Epoch: [7/10] Loss: 0.0573 ACC@1: 100.00% ACC@5: 100.00%: 100%|██████████| 4/4 [00:00<00:00,  5.77it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [7/10] Loss: 0.2767 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/2 [00:49<?, ?it/s]\u001B[A\n",
      "Test Epoch: [7/10] Loss: 0.2767 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 1/2 [00:49<00:49, 49.84s/it]\u001B[A\n",
      "Test Epoch: [7/10] Loss: 0.8535 ACC@1: 50.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:39<00:49, 49.84s/it] \u001B[A\n",
      "Test Epoch: [7/10] Loss: 0.8535 ACC@1: 50.00% ACC@5: 100.00%: 100%|██████████| 2/2 [01:39<00:00, 49.96s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [8/10] Loss: 0.0428 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [8/10] Loss: 0.0428 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.66it/s]\u001B[A\n",
      "Train Epoch: [8/10] Loss: 0.0400 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.66it/s]\u001B[A\n",
      "Train Epoch: [8/10] Loss: 0.0400 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.77it/s]\u001B[A\n",
      "Train Epoch: [8/10] Loss: 0.0408 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.77it/s]\u001B[A\n",
      "Train Epoch: [8/10] Loss: 0.0408 ACC@1: 100.00% ACC@5: 100.00%:  75%|███████▌  | 3/4 [00:00<00:00,  4.94it/s]\u001B[A\n",
      "Train Epoch: [8/10] Loss: 0.0411 ACC@1: 100.00% ACC@5: 100.00%: 100%|██████████| 4/4 [00:00<00:00,  5.97it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [8/10] Loss: 0.2514 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/2 [00:49<?, ?it/s]\u001B[A\n",
      "Test Epoch: [8/10] Loss: 0.2514 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 1/2 [00:49<00:49, 49.76s/it]\u001B[A\n",
      "Test Epoch: [8/10] Loss: 0.8829 ACC@1: 50.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:40<00:49, 49.76s/it] \u001B[A\n",
      "Test Epoch: [8/10] Loss: 0.8829 ACC@1: 50.00% ACC@5: 100.00%: 100%|██████████| 2/2 [01:40<00:00, 50.41s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [9/10] Loss: 0.0394 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [9/10] Loss: 0.0394 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.62it/s]\u001B[A\n",
      "Train Epoch: [9/10] Loss: 0.0383 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.62it/s]\u001B[A\n",
      "Train Epoch: [9/10] Loss: 0.0383 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.82it/s]\u001B[A\n",
      "Train Epoch: [9/10] Loss: 0.0360 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.82it/s]\u001B[A\n",
      "Train Epoch: [9/10] Loss: 0.0360 ACC@1: 100.00% ACC@5: 100.00%:  75%|███████▌  | 3/4 [00:00<00:00,  4.57it/s]\u001B[A\n",
      "Train Epoch: [9/10] Loss: 0.0361 ACC@1: 100.00% ACC@5: 100.00%: 100%|██████████| 4/4 [00:00<00:00,  5.58it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [9/10] Loss: 0.2320 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/2 [00:52<?, ?it/s]\u001B[A\n",
      "Test Epoch: [9/10] Loss: 0.2320 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 1/2 [00:52<00:52, 52.33s/it]\u001B[A\n",
      "Test Epoch: [9/10] Loss: 0.9101 ACC@1: 50.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:42<00:52, 52.33s/it] \u001B[A\n",
      "Test Epoch: [9/10] Loss: 0.9101 ACC@1: 50.00% ACC@5: 100.00%: 100%|██████████| 2/2 [01:42<00:00, 51.33s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [10/10] Loss: 0.0318 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Train Epoch: [10/10] Loss: 0.0318 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.13it/s]\u001B[A\n",
      "Train Epoch: [10/10] Loss: 0.0306 ACC@1: 100.00% ACC@5: 100.00%:  25%|██▌       | 1/4 [00:00<00:00,  4.13it/s]\u001B[A\n",
      "Train Epoch: [10/10] Loss: 0.0306 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.66it/s]\u001B[A\n",
      "Train Epoch: [10/10] Loss: 0.0296 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 2/4 [00:00<00:00,  4.66it/s]\u001B[A\n",
      "Train Epoch: [10/10] Loss: 0.0296 ACC@1: 100.00% ACC@5: 100.00%:  75%|███████▌  | 3/4 [00:00<00:00,  4.82it/s]\u001B[A\n",
      "Train Epoch: [10/10] Loss: 0.0294 ACC@1: 100.00% ACC@5: 100.00%: 100%|██████████| 4/4 [00:00<00:00,  5.66it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Test Epoch: [10/10] Loss: 0.2170 ACC@1: 100.00% ACC@5: 100.00%:   0%|          | 0/2 [00:50<?, ?it/s]\u001B[A\n",
      "Test Epoch: [10/10] Loss: 0.2170 ACC@1: 100.00% ACC@5: 100.00%:  50%|█████     | 1/2 [00:50<00:50, 50.70s/it]\u001B[A\n",
      "Test Epoch: [10/10] Loss: 0.9330 ACC@1: 50.00% ACC@5: 100.00%:  50%|█████     | 1/2 [01:42<00:50, 50.70s/it] \u001B[A\n",
      "Test Epoch: [10/10] Loss: 0.9330 ACC@1: 50.00% ACC@5: 100.00%: 100%|██████████| 2/2 [01:42<00:00, 51.07s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best top-1 accuracy with self-supervised learning:  50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 101
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
