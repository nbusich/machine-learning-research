{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b14b725c19e03afe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sim(z_i, z_j):\n",
    "    \"\"\"Normalized dot product between two vectors.\n",
    "\n",
    "    Inputs:\n",
    "    - z_i: 1xD tensor.\n",
    "    - z_j: 1xD tensor.\n",
    "    \n",
    "    Returns:\n",
    "    - A scalar value that is the normalized dot product between z_i and z_j.\n",
    "    \"\"\"\n",
    "    norm_dot_product = None\n",
    "    ##############################################################################\n",
    "    # TODO: Start of your code.                                                  #\n",
    "    #                                                                            #\n",
    "    # HINT: torch.linalg.norm might be helpful.                                  #\n",
    "    ##############################################################################\n",
    "    \n",
    "    num = z_i @ z_j\n",
    "    denom = torch.linalg.norm(z_i) * torch.linalg.norm(z_j)\n",
    "    norm_dot_product = num/denom\n",
    "\n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    \n",
    "    return norm_dot_product\n",
    "\n",
    "\n",
    "def simclr_loss_naive(out_left, out_right, tau):\n",
    "    \"\"\"Compute the contrastive loss L over a batch (naive loop version).\n",
    "    \n",
    "    Input:\n",
    "    - out_left: NxD tensor; output of the projection head g(), left branch in SimCLR model.\n",
    "    - out_right: NxD tensor; output of the projection head g(), right branch in SimCLR model.\n",
    "    Each row is a z-vector for an augmented sample in the batch. The same row in out_left and out_right form a positive pair. \n",
    "    In other words, (out_left[k], out_right[k]) form a positive pair for all k=0...N-1.\n",
    "    - tau: scalar value, temperature parameter that determines how fast the exponential increases.\n",
    "    \n",
    "    Returns:\n",
    "    - A scalar value; the total loss across all positive pairs in the batch. See notebook for definition.\n",
    "    \"\"\"\n",
    "    N = out_left.shape[0]  # total number of training examples\n",
    "    \n",
    "     # Concatenate out_left and out_right into a 2*N x D tensor.\n",
    "    out = torch.cat([out_left, out_right], dim=0)  # [2*N, D]\n",
    "    total_loss = 0\n",
    "    for k in range(N):  # loop through each positive pair (k, k+N)\n",
    "        z_k, z_k_N = out[k], out[k+N]\n",
    "        ##############################################################################\n",
    "        # TODO: Start of your code.                                                  #\n",
    "        #                                                                            #\n",
    "        # Hint: Compute l(k, k+N) and l(k+N, k).                                    #\n",
    "        ##############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        def l(i,j):\n",
    "          num = torch.exp(sim(i, j)/tau)\n",
    "          denom = 0.0\n",
    "          for o in range(2*N):\n",
    "            if k == o:\n",
    "              pass\n",
    "            else:\n",
    "              denom += np.exp((sim(i, out[o]))/tau)\n",
    "          return -torch.log(num/denom)\n",
    "\n",
    "        total_loss += l(z_k, z_k_N) + l(z_k_N, z_k)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "         ##############################################################################\n",
    "        #                               END OF YOUR CODE                             #\n",
    "        ##############################################################################\n",
    "    \n",
    "    # In the end, we need to divide the total loss by 2N, the number of samples in the batch.\n",
    "    total_loss = total_loss / (2*N)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def sim_positive_pairs(out_left, out_right):\n",
    "    \"\"\"Normalized dot product between positive pairs.\n",
    "\n",
    "    Inputs:\n",
    "    - out_left: NxD tensor; output of the projection head g(), left branch in SimCLR model.\n",
    "    - out_right: NxD tensor; output of the projection head g(), right branch in SimCLR model.\n",
    "    Each row is a z-vector for an augmented sample in the batch.\n",
    "    The same row in out_left and out_right form a positive pair.\n",
    "    \n",
    "    Returns:\n",
    "    - A Nx1 tensor; each row k is the normalized dot product between out_left[k] and out_right[k].\n",
    "    \"\"\"\n",
    "    pos_pairs = None\n",
    "\n",
    "    \n",
    "    \n",
    "    ##############################################################################\n",
    "    # TODO: Start of your code.                                                  #\n",
    "    #                                                                            #\n",
    "    # HINT: torch.linalg.norm might be helpful.                                  #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # Compute the normalized head projection outputs over each row of features\n",
    "    norm_left = out_left / torch.linalg.norm(out_left, dim=1, keepdim=True)\n",
    "    norm_right = out_right / torch.linalg.norm(out_right, dim=1, keepdim=True)\n",
    "\n",
    "    # Compute the diagonal dot product directly by multiplying and summing\n",
    "    pos_pairs = (norm_left * norm_right).sum(dim=1, keepdim=True)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    return pos_pairs\n",
    "\n",
    "\n",
    "def compute_sim_matrix(out):\n",
    "    \"\"\"Compute a 2N x 2N matrix of normalized dot products between all pairs of augmented examples in a batch.\n",
    "\n",
    "    Inputs:\n",
    "    - out: 2N x D tensor; each row is the z-vector (output of projection head) of a single augmented example.\n",
    "    There are a total of 2N augmented examples in the batch.\n",
    "    \n",
    "    Returns:\n",
    "    - sim_matrix: 2N x 2N tensor; each element i, j in the matrix is the normalized dot product between out[i] and out[j].\n",
    "    \"\"\"\n",
    "    sim_matrix = None\n",
    "    \n",
    "    ##############################################################################\n",
    "    # TODO: Start of your code.                                                  #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    norm_out = out / torch.linalg.norm(out, dim=1, keepdim=True)\n",
    "    sim_matrix = norm_out @ norm_out.T\n",
    "\n",
    "        \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "def simclr_loss_vectorized(out_left, out_right, tau, device='cuda'):\n",
    "    \"\"\"Compute the contrastive loss L over a batch (vectorized version). No loops are allowed.\n",
    "    \n",
    "    Inputs and output are the same as in simclr_loss_naive.\n",
    "    \"\"\"\n",
    "    N = out_left.shape[0]\n",
    "    \n",
    "    # Concatenate out_left and out_right into a 2*N x D tensor.\n",
    "    out = torch.cat([out_left, out_right], dim=0)  # [2*N, D]\n",
    "    \n",
    "    # Compute similarity matrix between all pairs of augmented examples in the batch.\n",
    "    sim_matrix = compute_sim_matrix(out)  # [2*N, 2*N]\n",
    "\n",
    "    ##############################################################################\n",
    "    # TODO: Start of your code. Follow the hints.                                #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # Step 1: Use sim_matrix to compute the denominator value for all augmented samples.\n",
    "    # Hint: Compute e^{sim / tau} and store into exponential, which should have shape 2N x 2N.\n",
    "    exponential = torch.exp(sim_matrix/tau)\n",
    "    \n",
    "    # This binary mask zeros out terms where k=i.\n",
    "    mask = (torch.ones_like(exponential, device=device) - torch.eye(2 * N, device=device)).to(device).bool()\n",
    "    \n",
    "    # We apply the binary mask.\n",
    "    exponential = exponential.masked_select(mask).view(2 * N, -1)  # [2*N, 2*N-1]\n",
    "    \n",
    "    # Hint: Compute the denominator values for all augmented samples. This should be a 2N x 1 vector.\n",
    "    denom = torch.sum(exponential, axis = 1)\n",
    "    \n",
    "\n",
    "    # Step 2: Compute similarity between positive pairs.\n",
    "    # You can do this in two ways: \n",
    "    # Option 1: Extract the corresponding indices from sim_matrix. \n",
    "    # Option 2: Use sim_positive_pairs().\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    similarity_vector = sim_positive_pairs(out_left, out_right) # shape (2,1)\n",
    "  \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # Step 3: Compute the numerator value for all augmented samples.\n",
    "    numerator = None\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    numerator = torch.exp(similarity_vector/tau)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # Step 4: Now that you have the numerator and denominator for all augmented samples, compute the total loss.\n",
    "    loss = None\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    loss = -torch.log(numerator/denom).mean()\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ],
   "id": "8d9c85203cc4b888"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
