{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9fe0fe3-8f40-4685-ac55-941ffd0961cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: torch in /Users/priyankaadhikari/anaconda3/lib/python3.12/site-packages (from thop) (2.6.0)\n",
      "Requirement already satisfied: filelock in /Users/priyankaadhikari/anaconda3/lib/python3.12/site-packages (from torch->thop) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/priyankaadhikari/anaconda3/lib/python3.12/site-packages (from torch->thop) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/priyankaadhikari/anaconda3/lib/python3.12/site-packages (from torch->thop) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/priyankaadhikari/anaconda3/lib/python3.12/site-packages (from torch->thop) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/priyankaadhikari/anaconda3/lib/python3.12/site-packages (from torch->thop) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /Users/priyankaadhikari/anaconda3/lib/python3.12/site-packages (from torch->thop) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/priyankaadhikari/anaconda3/lib/python3.12/site-packages (from torch->thop) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/priyankaadhikari/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/priyankaadhikari/anaconda3/lib/python3.12/site-packages (from jinja2->torch->thop) (2.1.3)\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: thop\n",
      "Successfully installed thop-0.1.1.post2209072238\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Setup cell.\n",
    "%pip install thop\n",
    "import torch\n",
    "import os\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from thop import profile, clever_format\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ead306-dbe9-4b75-b2e3-bfb887a0b22c",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Our first step is to perform data augmentation. Implement the compute_train_transform() function in cs231n/simclr/data_utils.py to apply the following random transformations:\n",
    "\n",
    "Randomly resize and crop to 32x32.\n",
    "Horizontally flip the image with probability 0.5\n",
    "With a probability of 0.8, apply color jitter (see compute_train_transform() for definition)\n",
    "With a probability of 0.2, convert the image to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f7d4600-3af7-4785-bab6-8a387f40d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de772c62-5264-492d-b42c-a611305d2d29",
   "metadata": {},
   "source": [
    "Now complete compute_train_transform() and CIFAR10Pair.__getitem__() in cs231n/simclr/data_utils.py to apply the data augmentation transform and generate 𝑥̃ 𝑖 and 𝑥̃ 𝑗.\n",
    "\n",
    "Test to make sure that your data augmentation code is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd42c33-d363-480d-947d-6c9207a10614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n.simclr.data_utils import *\n",
    "from cs231n.simclr.contrastive_loss import *\n",
    "\n",
    "answers = torch.load('simclr_sanity_check.key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671148c-02cb-4fb5-9d7e-ef2a12945f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "def test_data_augmentation(correct_output=None):\n",
    "    train_transform = compute_train_transform(seed=2147483647)\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=2, shuffle=False, num_workers=2)\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = next(dataiter)\n",
    "    img = torchvision.utils.make_grid(images)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    output = images\n",
    "\n",
    "    print(\"Maximum error in data augmentation: %g\"%rel_error( output.numpy(), correct_output.numpy()))\n",
    "\n",
    "# Should be less than 1e-07.\n",
    "test_data_augmentation(answers['data_augmentation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d2318-6614-43a3-a0a8-d8fbeedc8a90",
   "metadata": {},
   "source": [
    "Base Encoder and Projection Head\n",
    "The next steps are to apply the base encoder and projection head to the augmented samples 𝑥̃ 𝑖 and 𝑥̃ 𝑗.\n",
    "\n",
    "The base encoder 𝑓 extracts representation vectors for the augmented samples. The SimCLR paper found that using deeper and wider models improved performance and thus chose ResNet to use as the base encoder. The output of the base encoder are the representation vectors ℎ𝑖=𝑓(𝑥̃ 𝑖) and ℎ𝑗=𝑓(𝑥̃ 𝑗).\n",
    "\n",
    "The projection head 𝑔 is a small neural network that maps the representation vectors ℎ𝑖 and ℎ𝑗 to the space where the contrastive loss is applied. The paper found that using a nonlinear projection head improved the representation quality of the layer before it. Specifically, they used a MLP with one hidden layer as the projection head 𝑔. The contrastive loss is then computed based on the outputs 𝑧𝑖=𝑔(ℎ𝑖) and 𝑧𝑗=𝑔(ℎ𝑗).\n",
    "\n",
    "We provide implementations of these two parts in cs231n/simclr/model.py. Please skim through the file and make sure you understand the implementation.\n",
    "\n",
    "SimCLR: Contrastive Loss\n",
    "A mini-batch of 𝑁 training images yields a total of 2𝑁 data-augmented examples. For each positive pair (𝑖,𝑗) of augmented examples, the contrastive loss function aims to maximize the agreement of vectors 𝑧𝑖 and 𝑧𝑗. Specifically, the loss is the normalized temperature-scaled cross entropy loss and aims to maximize the agreement of 𝑧𝑖 and 𝑧𝑗 relative to all other augmented examples in the batch:\n",
    "\n",
    "𝑙(𝑖,𝑗)=−logexp(sim(𝑧𝑖,𝑧𝑗)/𝜏)∑2𝑁𝑘=1𝟙𝑘≠𝑖exp(sim(𝑧𝑖,𝑧𝑘)/𝜏)\n",
    "\n",
    "where 𝟙∈{0,1} is an indicator function that outputs 1 if 𝑘≠𝑖 and 0 otherwise. 𝜏 is a temperature parameter that determines how fast the exponentials increase.\n",
    "\n",
    "sim(𝑧𝑖,𝑧𝑗)=𝑧𝑖⋅𝑧𝑗||𝑧𝑖||||𝑧𝑗|| is the (normalized) dot product between vectors 𝑧𝑖 and 𝑧𝑗. The higher the similarity between 𝑧𝑖 and 𝑧𝑗, the larger the dot product is, and the larger the numerator becomes. The denominator normalizes the value by summing across 𝑧𝑖 and all other augmented examples 𝑘 in the batch. The range of the normalized value is (0,1), where a high score close to 1 corresponds to a high similarity between the positive pair (𝑖,𝑗) and low similarity between 𝑖 and other augmented examples 𝑘 in the batch. The negative log then maps the range (0,1) to the loss values (inf,0).\n",
    "\n",
    "The total loss is computed across all positive pairs (𝑖,𝑗) in the batch. Let 𝑧=[𝑧1,𝑧2,...,𝑧2𝑁] include all the augmented examples in the batch, where 𝑧1...𝑧𝑁 are outputs of the left branch, and 𝑧𝑁+1...𝑧2𝑁 are outputs of the right branch. Thus, the positive pairs are (𝑧𝑘,𝑧𝑘+𝑁) for ∀𝑘∈[1,𝑁].\n",
    "\n",
    "Then, the total loss 𝐿 is:\n",
    "\n",
    "𝐿=12𝑁∑𝑘=1𝑁[𝑙(𝑘,𝑘+𝑁)+𝑙(𝑘+𝑁,𝑘)]\n",
    "\n",
    "NOTE: this equation is slightly different from the one in the paper. We've rearranged the ordering of the positive pairs in the batch, so the indices are different. The rearrangement makes it easier to implement the code in vectorized form.\n",
    "\n",
    "We'll walk through the steps of implementing the loss function in vectorized form. Implement the functions sim, simclr_loss_naive in cs231n/simclr/contrastive_loss.py. Test your code by running the sanity checks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e58a0-a2f2-4325-8b53-6f45e7e490e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n.simclr.contrastive_loss import *\n",
    "answers = torch.load('simclr_sanity_check.key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eea4ee-435f-4434-8983-7a4deb5e4b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sim(left_vec, right_vec, correct_output):\n",
    "    output = sim(left_vec, right_vec).cpu().numpy()\n",
    "    print(\"Maximum error in sim: %g\"%rel_error(correct_output.numpy(), output))\n",
    "\n",
    "# Should be less than 1e-07.\n",
    "test_sim(answers['left'][0], answers['right'][0], answers['sim'][0])\n",
    "test_sim(answers['left'][1], answers['right'][1], answers['sim'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d909d65-9800-4483-836e-e1a4f027e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_naive(left, right, tau, correct_output):\n",
    "    naive_loss = simclr_loss_naive(left, right, tau).item()\n",
    "    print(\"Maximum error in simclr_loss_naive: %g\"%rel_error(correct_output, naive_loss))\n",
    "\n",
    "# Should be less than 1e-07.\n",
    "test_loss_naive(answers['left'], answers['right'], 5.0, answers['loss']['5.0'])\n",
    "test_loss_naive(answers['left'], answers['right'], 1.0, answers['loss']['1.0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68065117-bdbf-45ae-9d46-6ab72488da14",
   "metadata": {},
   "source": [
    "# Now implement the vectorized version\n",
    "Do this by implementing sim_positive_pairs, compute_sim_matrix, simclr_loss_vectorized in cs231n/simclr/contrastive_loss.py. Test your code by running the sanity checks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15633ea1-19d7-4947-9cae-033f55438381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sim_positive_pairs(left, right, correct_output):\n",
    "    sim_pair = sim_positive_pairs(left, right).cpu().numpy()\n",
    "    print(\"Maximum error in sim_positive_pairs: %g\"%rel_error(correct_output.numpy(), sim_pair))\n",
    "\n",
    "# Should be less than 1e-07.\n",
    "test_sim_positive_pairs(answers['left'], answers['right'], answers['sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99bab69-6a43-4b1b-a4cc-88dffed83e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sim_matrix(left, right, correct_output):\n",
    "    out = torch.cat([left, right], dim=0)\n",
    "    sim_matrix = compute_sim_matrix(out).cpu()\n",
    "    assert torch.isclose(sim_matrix, correct_output).all(), \"correct: {}. got: {}\".format(correct_output, sim_matrix)\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "test_sim_matrix(answers['left'], answers['right'], answers['sim_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fac46a-fb21-4c04-9303-f6b25afaee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_vectorized(left, right, tau, correct_output):\n",
    "    vec_loss = simclr_loss_vectorized(left, right, tau, device=left.device).item()\n",
    "    print(\"Maximum error in loss_vectorized: %g\"%rel_error(correct_output, vec_loss))\n",
    "\n",
    "# Should be less than 1e-07.\n",
    "test_loss_vectorized(answers['left'], answers['right'], 5.0, answers['loss']['5.0'])\n",
    "test_loss_vectorized(answers['left'], answers['right'], 1.0, answers['loss']['1.0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3e0f6-9a58-44c3-b2f5-61e860f2a91a",
   "metadata": {},
   "source": [
    "# Implement the train function\n",
    "Complete the train() function in cs231n/simclr/utils.py to obtain the model's output and use simclr_loss_vectorized to compute the loss. (Please take a look at the Model class in cs231n/simclr/model.py to understand the model pipeline and the returned values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026bd40e-e7c3-4e92-bc0f-ac858037f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n.simclr.data_utils import *\n",
    "from cs231n.simclr.model import *\n",
    "from cs231n.simclr.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7012528b-72e9-47e5-b7f2-8ee71818fa30",
   "metadata": {},
   "source": [
    "# Train the SimCLR model\n",
    "Run the following cells to load in the pretrained weights and continue to train a little bit more. This part will take ~10 minutes and will output to pretrained_model/trained_simclr_model.pth.\n",
    "\n",
    "NOTE: Don't worry about logs such as '[WARN] Cannot find rule for ...'. These are related to another module used in the notebook. You can verify the integrity of your code changes through our provided prompts and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09520bc8-0982-46be-b0b0-e273aba46359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell.\n",
    "feature_dim = 128\n",
    "temperature = 0.5\n",
    "k = 200\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "temperature = 0.5\n",
    "percentage = 0.5\n",
    "pretrained_path = './pretrained_model/pretrained_simclr_model.pth'\n",
    "\n",
    "# Prepare the data.\n",
    "train_transform = compute_train_transform()\n",
    "train_data = CIFAR10Pair(root='data', train=True, transform=train_transform, download=True)\n",
    "train_data = torch.utils.data.Subset(train_data, list(np.arange(int(len(train_data)*percentage))))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n",
    "test_transform = compute_test_transform()\n",
    "memory_data = CIFAR10Pair(root='data', train=True, transform=test_transform, download=True)\n",
    "memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "test_data = CIFAR10Pair(root='data', train=False, transform=test_transform, download=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "# Set up the model and optimizer config.\n",
    "model = Model(feature_dim)\n",
    "model.load_state_dict(torch.load(pretrained_path, map_location='cpu'), strict=False)\n",
    "model = model.to(device)\n",
    "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).to(device),))\n",
    "flops, params = clever_format([flops, params])\n",
    "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "c = len(memory_data.classes)\n",
    "\n",
    "# Training loop.\n",
    "results = {'train_loss': [], 'test_acc@1': [], 'test_acc@5': []} #<< -- output\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, epoch, epochs, batch_size=batch_size, temperature=temperature, device=device)\n",
    "    results['train_loss'].append(train_loss)\n",
    "    test_acc_1, test_acc_5 = test(model, memory_loader, test_loader, epoch, epochs, c, k=k, temperature=temperature, device=device)\n",
    "    results['test_acc@1'].append(test_acc_1)\n",
    "    results['test_acc@5'].append(test_acc_5)\n",
    "\n",
    "    # Save statistics.\n",
    "    if test_acc_1 > best_acc:\n",
    "        best_acc = test_acc_1\n",
    "        torch.save(model.state_dict(), './pretrained_model/trained_simclr_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afb60f-0b54-4823-b7d0-85603ce833e1",
   "metadata": {},
   "source": [
    "# Finetune a Linear Layer for Classification!\n",
    "Now it's time to put the representation vectors to the test!\n",
    "\n",
    "We remove the projection head from the SimCLR model and slap on a linear layer to finetune for a simple classification task. All layers before the linear layer are frozen, and only the weights in the final linear layer are trained. We compare the performance of the SimCLR + finetuning model against a baseline model, where no self-supervised learning is done beforehand, and all weights in the model are trained. You will get to see for yourself the power of self-supervised learning and how the learned representation vectors improve downstream task performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d926698-f4df-4773-a6cd-a50f5172e5c2",
   "metadata": {},
   "source": [
    "# Baseline: Without Self-Supervised Learning\n",
    "First, let's take a look at the baseline model. We'll remove the projection head from the SimCLR model and slap on a linear layer to finetune for a simple classification task. No self-supervised learning is done beforehand, and all weights in the model are trained. Run the following cells.\n",
    "\n",
    "NOTE: Don't worry if you see low but reasonable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf69e3c-17a4-4448-9b72-e135b98ee10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # Encoder.\n",
    "        self.f = Model().f\n",
    "\n",
    "        # Classifier.\n",
    "        self.fc = nn.Linear(2048, num_class, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        out = self.fc(feature)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd44ed-6913-4738-b5e5-2b40d41e8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell.\n",
    "feature_dim = 128\n",
    "temperature = 0.5\n",
    "k = 200\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "percentage = 0.1\n",
    "\n",
    "train_transform = compute_train_transform()\n",
    "train_data = CIFAR10(root='data', train=True, transform=train_transform, download=True)\n",
    "trainset = torch.utils.data.Subset(train_data, list(np.arange(int(len(train_data)*percentage))))\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "test_transform = compute_test_transform()\n",
    "test_data = CIFAR10(root='data', train=False, transform=test_transform, download=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "model = Classifier(num_class=len(train_data.classes)).to(device)\n",
    "for param in model.f.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).to(device),))\n",
    "flops, params = clever_format([flops, params])\n",
    "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "no_pretrain_results = {'train_loss': [], 'train_acc@1': [], 'train_acc@5': [],\n",
    "           'test_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_acc_1, train_acc_5 = train_val(model, train_loader, optimizer, epoch, epochs, device='cuda')\n",
    "    no_pretrain_results['train_loss'].append(train_loss)\n",
    "    no_pretrain_results['train_acc@1'].append(train_acc_1)\n",
    "    no_pretrain_results['train_acc@5'].append(train_acc_5)\n",
    "    test_loss, test_acc_1, test_acc_5 = train_val(model, test_loader, None, epoch, epochs)\n",
    "    no_pretrain_results['test_loss'].append(test_loss)\n",
    "    no_pretrain_results['test_acc@1'].append(test_acc_1)\n",
    "    no_pretrain_results['test_acc@5'].append(test_acc_5)\n",
    "    if test_acc_1 > best_acc:\n",
    "        best_acc = test_acc_1\n",
    "\n",
    "# Print the best test accuracy.\n",
    "print('Best top-1 accuracy without self-supervised learning: ', best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584653d3-57d5-44b0-8a23-6ef0cbccdd65",
   "metadata": {},
   "source": [
    "# With Self-Supervised Learning\n",
    "Let's see how much improvement we get with self-supervised learning. Here, we pretrain the SimCLR model using the simclr loss you wrote, remove the projection head from the SimCLR model, and use a linear layer to finetune for a simple classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7b15b-6a90-4c70-b5e1-0ba18198227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell.\n",
    "feature_dim = 128\n",
    "temperature = 0.5\n",
    "k = 200\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "percentage = 0.1\n",
    "pretrained_path = './pretrained_model/trained_simclr_model.pth'\n",
    "\n",
    "train_transform = compute_train_transform()\n",
    "train_data = CIFAR10(root='data', train=True, transform=train_transform, download=True)\n",
    "trainset = torch.utils.data.Subset(train_data, list(np.arange(int(len(train_data)*percentage))))\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "test_transform = compute_test_transform()\n",
    "test_data = CIFAR10(root='data', train=False, transform=test_transform, download=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "model = Classifier(num_class=len(train_data.classes))\n",
    "model.load_state_dict(torch.load(pretrained_path, map_location='cpu'), strict=False)\n",
    "model = model.to(device)\n",
    "for param in model.f.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).to(device),))\n",
    "flops, params = clever_format([flops, params])\n",
    "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "pretrain_results = {'train_loss': [], 'train_acc@1': [], 'train_acc@5': [],\n",
    "           'test_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_acc_1, train_acc_5 = train_val(model, train_loader, optimizer, epoch, epochs)\n",
    "    pretrain_results['train_loss'].append(train_loss)\n",
    "    pretrain_results['train_acc@1'].append(train_acc_1)\n",
    "    pretrain_results['train_acc@5'].append(train_acc_5)\n",
    "    test_loss, test_acc_1, test_acc_5 = train_val(model, test_loader, None, epoch, epochs)\n",
    "    pretrain_results['test_loss'].append(test_loss)\n",
    "    pretrain_results['test_acc@1'].append(test_acc_1)\n",
    "    pretrain_results['test_acc@5'].append(test_acc_5)\n",
    "    if test_acc_1 > best_acc:\n",
    "        best_acc = test_acc_1\n",
    "\n",
    "# Print the best test accuracy. You should see a best top-1 accuracy of >=70%.\n",
    "print('Best top-1 accuracy with self-supervised learning: ', best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2037b-2fbe-4566-ac0d-eef3227eef39",
   "metadata": {},
   "source": [
    "# Plot your Comparison\n",
    "Plot the test accuracies between the baseline model (no pretraining) and same model pretrained with self-supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c666468-47bb-46bb-8df2-3dd92bbb6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(no_pretrain_results['test_acc@1'], label=\"Without Pretrain\")\n",
    "plt.plot(pretrain_results['test_acc@1'], label=\"With Pretrain\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Top-1 Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
